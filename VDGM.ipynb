{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58e73df",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno, se aborda el problema de la clasificación supervisada utilizando el algoritmo Naive Bayes. El enfoque se centra en trabajar con una base de datos que contiene variables continuas y una variable objetivo denominada \"clase\", que representa las etiquetas de clasificación.\n",
    "\n",
    "El desafío principal al aplicar Naive Bayes a datos continuos se encuentra en la suposición del algoritmo de que las características son discretas. Para efrentar este reto, nuestro primer paso será discretizar las variables continuas utilizando el algoritmo CAIM (Class-Attribute Interdependence Maximization). Este método de discretización busca maximizar la dependencia entre las características y la clase objetivo, lo que resulta en una representación en forma de pertenencia a un intervalo para que sea legible en el clasificador Naive Bayes.\n",
    "\n",
    "Una vez discretizados los datos, se implementa el clasificador Naive Bayes. Para evaluar la eficacia del modelo, se utiliza la técnica de validación cruzada K-Fold. Esto permite estimar la precisión del clasificador al dividir el conjunto de datos en 'K' subconjuntos, entrenando el modelo en 'K-1' de ellos y validando el rendimiento en el resto. Repitiendo este proceso 'K' veces y promediando los resultados, obtenemos una evaluación más robusta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ad4e",
   "metadata": {},
   "source": [
    "Primero hay que importar algunas librerias de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e02f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c06ba8",
   "metadata": {},
   "source": [
    "## CAIM (Class-Attribute Interdependence Maximization)\n",
    "\n",
    "El algoritmo CAIM (Class-Attribute Interdependence Maximization) es una técnica de discretización utilizada para convertir variables continuas en categóricas. Este algoritmo busca maximizar la dependencia entre los atributos y la clase objetivo, lo que resulta en una mejor separación de las clases que conlleva a una mejora del rendimiento del clasificador.\n",
    "\n",
    "En el artículo CAIM Discretization Algorithm de Lukasz A. and Kurgan Krzysztof J. Cios se muestra un pseudocódigo del proceso, en matlab ya se encuentra una implementación dividida en 3 archivos con funciones complementarias. Tomando como referencia el artículo y la implementación en matlab (https://la.mathworks.com/matlabcentral/fileexchange/24344-caim-discretization-algorithm) se muestra una implementación en python:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8f15b",
   "metadata": {},
   "source": [
    "## Funcionamiento del Algoritmo CAIM:\n",
    "* Inicialización: El algoritmo comienza con la creación de un esquema de división inicial para cada variable continua, que sólo contiene los valores mínimos y máximos de la característica.\n",
    "\n",
    "* Búsqueda de Puntos de División: Para cada variable, el algoritmo busca puntos de división potenciales entre los valores únicos de la característica, excluyendo el mínimo y el máximo.\n",
    "\n",
    "* Evaluación de Esquemas de División: Para cada punto de división potencial, se crea un nuevo esquema de división y se evalúa su CAIM. El CAIM es una medida que cuantifica qué tan buena es la división basada en la proporción de la clase mayoritaria en cada intervalo del esquema de división, puede interpretarse como la probabilidad de cada valor en aportar a la división.\n",
    "\n",
    "* Selección del Mejor Esquema: El algoritmo selecciona el esquema de división que maximiza el CAIM. Si el CAIM mejora lo suficiente entonces se actualiza el esquema de división; de lo contrario se detiene la búsqueda para esa característica.\n",
    "\n",
    "* Discretización: Una vez que se han determinado los esquemas de división para todas las variables, los datos se discretizan reemplazando los valores continuos por etiquetas categóricas que representan los intervalos a los que pertenecen definidos por los esquemas de división.\n",
    "\n",
    "## Implementación en el Código:\n",
    "\n",
    "La clase CAIMD implementa el algoritmo CAIM. Se puede inicializar con una lista de características categóricas o dejar que el algoritmo determine automáticamente cuáles deben ser tratadas como categóricas. Para este caso ya consideramos que la última columna representa las clases\n",
    "\n",
    "* El método fit se encarga de encontrar los esquemas de división óptimos para cada variable continua. Utiliza métodos auxiliares como get_caim para calcular el CAIM de un esquema de división e index_from_scheme para convertir un esquema de división en índices de división.\n",
    "    \n",
    "* El método transform se utiliza para discretizar un conjunto de datos basado en los esquemas de división encontrados durante el ajuste. Reemplaza los valores continuos en cada variable por etiquetas categóricas que representan los intervalos del esquema de división.\n",
    "\n",
    "* El método fit_transform es una combinación de fit y transform para ajustar el modelo y luego transformar los datos en una sola operación.\n",
    "\n",
    "    Se definen dos excepciones personalizadas, CategoricalParamException y NotEnoughPoints, para manejar situaciones específicas como parámetros incorrectos o características con valores insuficientes para la discretización.\n",
    "\n",
    "Esta implementación permite discretizar características continuas de manera efectiva, preparándolas para su uso con clasificadores que requieren características categóricas, como Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc144e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAIMD(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, categorical_features='auto'):\n",
    "        if isinstance(categorical_features, str):\n",
    "            self._features = categorical_features\n",
    "            self.categorical = None\n",
    "        elif (isinstance(categorical_features, list)) or (isinstance(categorical_features, np.ndarray)):\n",
    "            self._features = None\n",
    "            self.categorical = categorical_features\n",
    "        else:\n",
    "            raise CategoricalParamException(\n",
    "                \"valor incorrecto de'categorical_features'. Ingrese 'auto', una matriz de índices o etiquetas.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ## Almacena los esquemas de división por características\n",
    "        self.split_scheme = dict()\n",
    "        print(self.split_scheme)\n",
    "        if isinstance(X, pd.DataFrame ):\n",
    "            if isinstance(self._features, list):\n",
    "                self.categorical = [X.columns.get_loc(label) for label in self._features]\n",
    "            X = X.values\n",
    "            y = y.values\n",
    "        if self._features == 'auto':\n",
    "            self.categorical = self.check_categorical(X, y)\n",
    "        categorical = self.categorical\n",
    "        print('Categorical', categorical)\n",
    "\n",
    "        min_splits = np.unique(y).shape[0]\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            if j in categorical:\n",
    "                continue\n",
    "            xj = X[:, j]\n",
    "            xj = xj[np.invert(np.isnan(xj))]\n",
    "            new_index = xj.argsort()\n",
    "            xj = xj[new_index]\n",
    "            yj = y[new_index]\n",
    "            allsplits = np.unique(xj)[1:-1].tolist()  # potential split points\n",
    "            global_caim = -1\n",
    "            mainscheme = [xj[0], xj[-1]]\n",
    "            best_caim = 0\n",
    "            k = 1\n",
    "            while (k <= min_splits) or ((global_caim < best_caim) and (allsplits)):\n",
    "                split_points = np.random.permutation(allsplits).tolist()\n",
    "                best_scheme = None\n",
    "                best_point = None\n",
    "                best_caim = 0\n",
    "                k = k + 1\n",
    "                #print(f\"k ahora vale {k}\")\n",
    "                while split_points:\n",
    "                    scheme = mainscheme[:]\n",
    "                    sp = split_points.pop()\n",
    "                    scheme.append(sp)\n",
    "                    scheme.sort()\n",
    "                    c = self.get_caim(scheme, xj, yj)\n",
    "                    #print(f\"probando con el scheme {scheme} con valor caim {c}\\n\")\n",
    "                    if c > best_caim:\n",
    "                        #print(f\"el nuevo caim es mejor y ahora best_scheme es {scheme}\")\n",
    "                        best_caim = c\n",
    "                        best_scheme = scheme\n",
    "                        best_point = sp\n",
    "                    \n",
    "                if (k <= min_splits) or (best_caim > global_caim):\n",
    "                    print(\"algo pasó\")\n",
    "                    mainscheme = best_scheme\n",
    "                    global_caim = best_caim\n",
    "                    try:\n",
    "                        allsplits.remove(best_point)\n",
    "                    except ValueError:\n",
    "                        raise NotEnoughPoints('La variable' + str(j) + ' no cuenta con valores suficientes para discretizar')\n",
    "\n",
    "            self.split_scheme[j] = mainscheme\n",
    "            print('#', j, ' GLOBAL CAIM ', global_caim)\n",
    "            print(self.split_scheme)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.indx = X.index\n",
    "            self.columns = X.columns\n",
    "            X = X.values\n",
    "        X_di = X.copy()\n",
    "        categorical = self.categorical\n",
    "\n",
    "        scheme = self.split_scheme\n",
    "        for j in range(X.shape[1]):\n",
    "            if j in categorical:\n",
    "                continue\n",
    "            sh = scheme[j]\n",
    "            sh[-1] = sh[-1] + 1\n",
    "            xj = X[:, j]\n",
    "            # xi = xi[np.invert(np.isnan(xi))]\n",
    "            for i in range(len(sh) - 1):\n",
    "                ind = np.where((xj >= sh[i]) & (xj < sh[i + 1]))[0]\n",
    "                X_di[ind, j] = i\n",
    "        if hasattr(self, 'indx'):\n",
    "            return pd.DataFrame(X_di, index=self.indx, columns=self.columns)\n",
    "        return X_di\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def get_caim(self, scheme, xi, y):\n",
    "        sp = self.index_from_scheme(scheme[1:-1], xi)\n",
    "        sp.insert(0, 0)\n",
    "        sp.append(xi.shape[0])\n",
    "        n = len(sp) - 1\n",
    "        isum = 0\n",
    "        for j in range(n):\n",
    "            init = sp[j]\n",
    "            fin = sp[j + 1]\n",
    "            Mr = xi[init:fin].shape[0]\n",
    "            val, counts = np.unique(y[init:fin], return_counts=True)\n",
    "            maxr = counts.max()\n",
    "            isum = isum + (maxr / Mr) * maxr\n",
    "        return isum / n\n",
    "\n",
    "    def index_from_scheme(self, scheme, x_sorted):\n",
    "        split_points = []\n",
    "        for p in scheme:\n",
    "            split_points.append(np.where(x_sorted > p)[0][0])\n",
    "        return split_points\n",
    "\n",
    "    def check_categorical(self, X, y):\n",
    "        categorical = []\n",
    "        ny2 = 2 * np.unique(y).shape[0]\n",
    "        for j in range(X.shape[1]):\n",
    "            xj = X[:, j]\n",
    "            xj = xj[np.invert(np.isnan(xj))]\n",
    "            if np.unique(xj).shape[0] < ny2:\n",
    "                categorical.append(j)\n",
    "        return categorical\n",
    "\n",
    "\n",
    "class CategoricalParamException(Exception):\n",
    "    # Raise if wrong type of parameter\n",
    "    pass\n",
    "\n",
    "\n",
    "class NotEnoughPoints(Exception):\n",
    "    # Raise if a feature must be categorical, not continuous\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36881588",
   "metadata": {},
   "source": [
    "# Descripción del Clasificador Naive Bayes\n",
    "El clasificador Naive Bayes es un algoritmo de aprendizaje automático supervisado basado en el teorema de Bayes. Ha mostrado ser eficiente para la clasificación de datos con características categóricas con la ventaja de ser simple y eficiente: Asume independencia condicional entre las características, lo que significa que el efecto de una característica en una clase es independiente del efecto de otras características evitando el cálculo de probabilidades combinando todas las variables.\n",
    "\n",
    "## Funcionamiento del Clasificador Naive Bayes:\n",
    "\n",
    "* Entrenamiento (Método fit): Durante la fase de entrenamiento, el clasificador calcula la probabilidad a priori de cada clase (la probabilidad de que un punto de datos pertenezca a una clase dada antes de observar las características) y las probabilidades de verosimilitud de las características dadas las clases (la probabilidad de observar una característica dada una clase).\n",
    "$$P(y=c)=\\frac{ registros \\ con \\ clase c}{total\\ de\\ datos}$$\n",
    "$$P(x_i=v|y=c)= \\frac{Elementos \\ de \\ la \\ clase\\ c \\ con \\ valor \\ x_i =v}{Datos\\ con\\ clase\\ c} $$\n",
    "\n",
    "* Cálculo de Probabilidades de Verosimilitud (Método calculate_likelihood): Para cada característica, se calcula la probabilidad de verosimilitud como la frecuencia relativa de cada valor de la característica dentro de cada clase.\n",
    "\n",
    "* Predicción (Método predict): Para un nuevo punto de datos, el clasificador calcula la probabilidad posterior de cada clase, que es proporcional al producto de la probabilidad a priori de la clase y las probabilidades de verosimilitud de las características observadas. La clase con la mayor probabilidad posterior se selecciona como la predicción.\n",
    "\n",
    "$$P(y=c|x)\\simeq P(y=c)\\times \\prod_{i=1}^n P(x_i|y=c) $$\n",
    "\n",
    "## Validación Cruzada K-Fold\n",
    "Para el split en training set y test set se utiliza validación cruzada k-fold: éste es una técnica utilizada para evaluar la eficacia de un modelo de aprendizaje automático y asegurar que es generalizable a nuevos registros. Consiste en dividir el conjunto de datos en 'K' subconjuntos (o \"folds\") de aproximadamente igual tamaño. El proceso se realiza de la siguiente manera:\n",
    "\n",
    "* División de Datos: El conjunto de datos se divide aleatoriamente en 'K' subconjuntos o folds.\n",
    "\n",
    "* Entrenamiento y Validación: En cada iteración, se utiliza un fold diferente como conjunto de validación, mientras que los 'K-1' folds restantes se utilizan como conjunto de entrenamiento. El modelo se entrena en el conjunto de entrenamiento y se valida en el conjunto de validación.\n",
    "\n",
    "* Repetición: Este proceso se repite 'K' veces, de modo que cada fold se utiliza exactamente una vez como conjunto de validación.\n",
    "\n",
    "Promedio de Resultados: Los resultados de las 'K' iteraciones se promedian para obtener una estimación más precisa del rendimiento del modelo.\n",
    "\n",
    "\n",
    "\n",
    "## Implementación en el Código:\n",
    "\n",
    "La clase NaiveBayes implementa el clasificador Naive Bayes para características categóricas.\n",
    "\n",
    "* El método fit calcula y almacena las probabilidades a priori y de verosimilitud para cada clase y característica en el diccionario parameters. Las probabilidades a priori se calculan como la proporción de puntos de datos en cada clase, y las probabilidades de verosimilitud se calculan utilizando el método calculate_likelihood.\n",
    "\n",
    "* El método calculate_likelihood calcula la probabilidad de verosimilitud de una característica como la frecuencia relativa de cada valor de la característica dentro de la clase.\n",
    "\n",
    "* El método predict utiliza las probabilidades a priori y de verosimilitud almacenadas para calcular la probabilidad posterior de cada clase para un nuevo punto de datos. La clase con la mayor probabilidad posterior se selecciona como la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46136e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            self.parameters[cls] = {\n",
    "                'apriori': len(X_cls) / len(X),\n",
    "                'verosim': {i: self.calculate_likelihood(X_cls[:, i]) for i in range(X.shape[1])}\n",
    "            }\n",
    "\n",
    "    def calculate_likelihood(self, feature):\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        return dict(zip(values, counts / len(feature)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            for cls in self.classes:\n",
    "                prior = self.parameters[cls]['apriori']\n",
    "                likelihood = np.prod([self.parameters[cls]['verosim'][i].get(val, 1e-6) for i, val in enumerate(x)])\n",
    "                posteriors.append(prior * likelihood)\n",
    "            y_pred.append(self.classes[np.argmax(posteriors)])\n",
    "        print(y_pred)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55035ea0",
   "metadata": {},
   "source": [
    "## El algoritmo en acción\n",
    "\n",
    "Ahora, en la carpeta se encuentran 10 bases de datos a las cuales se puede acceder:\n",
    "* bupa.csv \n",
    "    * Este conjunto de datos contiene información sobre pacientes que han sido sometidos a pruebas para detectar trastornos hepáticos. Tiene 6 variables continuas, que incluyen varias pruebas de función hepática como las concentraciones de enzimas en la sangre, y una variable de clase binaria que indica la presencia o ausencia de trastornos hepáticos.\n",
    "* diabetes.csv \n",
    "    * Este conjunto de datos contiene información médica y de diagnóstico sobre pacientes femeninas de ascendencia indígena Pima. Tiene 8 variables continuas, como el número de embarazos, la concentración de glucosa en plasma, la presión arterial diastólica, y una variable de clase que indica si la paciente tiene diabetes.\n",
    "* sonar.csv \n",
    "    * Este conjunto de datos contiene patrones de señales obtenidos al rebotar señales de sonar en diferentes superficies. Tiene 60 variables continuas que representan la energía en diferentes bandas de frecuencia, y una variable de clase que indica si la señal corresponde a una mina o a una roca.\n",
    "* banknote.csv \n",
    "    * Este conjunto de datos contiene características extraídas de imágenes de billetes, como la variabilidad de la imagen, la entropía, etc. Tiene 4 variables continuas y una variable de clase que indica si el billete es auténtico o falso.\n",
    "* heart.csv \n",
    "    * Este conjunto de datos contiene información sobre pacientes con enfermedades del corazón. Tiene 13 variables continuas, como la edad, el colesterol sérico, la presión arterial en reposo, y una variable de clase que indica la presencia de enfermedad cardíaca. (para este caso eliminé las variables discretas porque noté que el algoritmo las vuelve a modificar)\n",
    "\n",
    "* iris.csv\n",
    "    * El conjunto de datos Iris tiene cuatro variables continuas (longitud y ancho del sépalo, longitud y ancho del pétalo) y una variable de clase con tres categorías (especies de iris).\n",
    "    \n",
    "* musk_clean.csv\n",
    "    * Este conjunto de datos describe un conjunto de 92 moléculas, con 162 variables, de las cuales 47 son consideradas almizcles por expertos humanos y las 45 restantes son consideradas no almizcles.  El objetivo es aprender a predecir si nuevas moléculas serán o no almizcles.\n",
    "* rice.csv\n",
    "    * Entre los arroces certificados cultivados en TURQUÍA, se han seleccionado para el estudio la especie Osmancik y la especie Cammeo, cultivada desde 2014.  Al observar las características generales de la especie Osmancik, éstas tienen un aspecto ancho, largo, vidrioso y opaco.  En cuanto a las características generales de la especie Cammeo, son anchas y largas, vidriosas y sin brillo.  Se tomó un total de 3.810 imágenes de granos de arroz de las dos especies, se procesaron y se hicieron inferencias de características. Se obtuvieron 7 rasgos morfológicos para cada grano de arroz\n",
    "* sonar.csv\n",
    "    * Este conjunto de datos contiene patrones de señales obtenidos al rebotar señales de sonar en diferentes superficies. Tiene 60 variables continuas que representan la energía en diferentes bandas de frecuencia, y una variable de clase que indica si la señal corresponde a una mina o a una roca.\n",
    "* vehicle.csv\n",
    "    * este conjunto de datos se utiliza para la clasificación de vehículos en cuatro categorías (autobús, furgoneta, automóvil, camión) basada en las características extraídas de las siluetas de los vehículos. Tiene 18 variables continuas, como el área de la silueta, la longitud máxima de la silueta, el ancho máximo de la silueta, la relación entre el área de la silueta y el cuadrado de la longitud de la silueta, y más. La variable de clase indica el tipo de vehículo. Para esta base decidí quitar algunas columnas que, aunque eran continuas, eran reescalamientos de otras variables.\n",
    "\n",
    "* wine.csv\n",
    "    * Este conjunto de datos contiene la información química de diferentes vinos. Tiene 13 variables continuas relacionadas con el contenido químico del vino y una variable de clase que indica el tipo de vino.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b1954",
   "metadata": {},
   "source": [
    "Sólo hay que cambiar el nombre de la base de datos y ejecutar todas las celdas de abajo para obtener el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffd0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"heart.csv\" #cambiar el nombre a alguna otra base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66455ec",
   "metadata": {},
   "source": [
    "Primero se hace la discretización usando CAIM. Se especifica las variables y la clase target, se asume que la clase es la última columna.\n",
    "\n",
    "Se hace un shuffle para aumentar la probabilidad de que el training set tenga al menos un elemento de cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a44dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(csv_file) \u001b[38;5;66;03m# leer la base de datos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m columnas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;66;03m# obtener la lista de las variables\u001b[39;00m\n\u001b[0;32m      4\u001b[0m variables \u001b[38;5;241m=\u001b[39m columnas[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# todas las variables menos la última que es la clase\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv_file) # leer la base de datos\n",
    "\n",
    "columnas = list(data.columns) # obtener la lista de las variables\n",
    "variables = columnas[:-1] # todas las variables menos la última que es la clase\n",
    "clase = columnas[-1] # la útlima columna es la clase\n",
    "\n",
    "print(\"las variables son:\")\n",
    "for variable in variables:\n",
    "    print(f\"{variable}\\n\")\n",
    "print(\"las clases son\")\n",
    "print(f\"{data[clase].unique()}\\n\")\n",
    "\n",
    "X = data[variables] # variables\n",
    "y = data[clase] # target\n",
    "               ###########################################\n",
    "X= np.array(X) # la clase NaiveBayes recibe arrays       #\n",
    "y= np.array(y) # por eso hay que cambiar el tipo de dato #\n",
    "               ###########################################\n",
    "    \n",
    "X, y = shuffle(X, y, random_state=42) #se hace un shuffle para asegurar que training set tenga todas las clases\n",
    "\n",
    "caim = CAIMD()  # se llama la clase caim\n",
    "x_disc = caim.fit_transform(X, y)  # se discretiza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93b36d",
   "metadata": {},
   "source": [
    "Ahora vemos la base de datos original y en seguida la base discretizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdcbc6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(X[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6a1b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>150</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>187</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>172</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>178</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>163</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>123</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>132</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>141</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>115</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  trestbps  chol  thalach  oldpeak  target\n",
       "0     63       145   233      150      2.3       1\n",
       "1     37       130   250      187      3.5       1\n",
       "2     41       130   204      172      1.4       1\n",
       "3     56       120   236      178      0.8       1\n",
       "4     57       120   354      163      0.6       1\n",
       "..   ...       ...   ...      ...      ...     ...\n",
       "298   57       140   241      123      0.2       0\n",
       "299   45       110   264      132      1.2       0\n",
       "300   68       144   193      141      3.4       0\n",
       "301   57       130   131      115      1.2       0\n",
       "302   57       130   236      174      0.0       0\n",
       "\n",
       "[303 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ec2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(x_disc, columns=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d053c315",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_disc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_disc\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_disc' is not defined"
     ]
    }
   ],
   "source": [
    "print(x_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9f744",
   "metadata": {},
   "source": [
    "Ahora se inicia un clasificador NaiveBayes y un kFolds con un split. Además se muestra la matriz de confusión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0967553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "Matriz de confusión:\n",
      "[[31 11]\n",
      " [16 43]]\n",
      "\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "Matriz de confusión:\n",
      "[[34 16]\n",
      " [17 34]]\n",
      "\n",
      "[0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "Matriz de confusión:\n",
      "[[32 14]\n",
      " [13 42]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayes()  # se crea el objeto con el que se va a entrenar\n",
    "kf = KFold(n_splits=3) # se describe cómo se hara el split en kfolds\n",
    "\n",
    "accuracies = []  #aquí guardamos la eficacia para hacer promedio\n",
    "confusion_matrices = [] #aquí guardamos la matriz de confución de cada iteración\n",
    "\n",
    "# Validación cruzada\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = x_disc[train_index], x_disc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "    print(f\"Matriz de confusión:\\n{cm}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd7446",
   "metadata": {},
   "source": [
    "Ahora vemos el promedio de de precisión de cada fold y su desviación estandar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "562331b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión promedio: 0.712871287128713\n",
      "Desviación estándar de la presición: 0.028004228957883043\n"
     ]
    }
   ],
   "source": [
    "average_precision = np.mean(accuracies)\n",
    "std_precision = np.std(accuracies)\n",
    "\n",
    "print(\"Precisión promedio:\", average_precision)\n",
    "print(\"Desviación estándar de la presición:\", std_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cee6a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'apriori': 0.45544554455445546,\n",
       "  'verosim': {0: {0.0: 0.2608695652173913, 1.0: 0.7391304347826086},\n",
       "   1: {0.0: 0.717391304347826, 1.0: 0.2826086956521739},\n",
       "   2: {0.0: 0.5978260869565217, 1.0: 0.40217391304347827},\n",
       "   3: {0.0: 0.6413043478260869, 1.0: 0.358695652173913},\n",
       "   4: {0.0: 0.2826086956521739, 1.0: 0.717391304347826}}},\n",
       " 1: {'apriori': 0.5445544554455446,\n",
       "  'verosim': {0: {0.0: 0.5545454545454546, 1.0: 0.44545454545454544},\n",
       "   1: {0.0: 0.8363636363636363, 1.0: 0.16363636363636364},\n",
       "   2: {0.0: 0.7727272727272727, 1.0: 0.22727272727272727},\n",
       "   3: {0.0: 0.2, 1.0: 0.8},\n",
       "   4: {0.0: 0.6454545454545455, 1.0: 0.35454545454545455}}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c80ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302]\n",
      "  Test:  index=[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100]\n",
      "Fold 1:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302]\n",
      "  Test:  index=[101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201]\n",
      "Fold 2:\n",
      "  Train: index=[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201]\n",
      "  Test:  index=[202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\n",
      " 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
      " 292 293 294 295 296 297 298 299 300 301 302]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d91012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beadeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4381bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
